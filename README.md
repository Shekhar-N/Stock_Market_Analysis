ğŸ“ˆ Stock Market Near Real-Time Analysis using PySpark
ğŸ“Œ Project Overview

This project implements a near real-time stock market analytics pipeline using PySpark Structured Streaming and open-source tools only.

Due to the lack of free true real-time stock exchange feeds, the system uses near real-time data ingestion and a file-based streaming architecture to simulate a real-world streaming pipeline. This approach is widely accepted for learning and academic projects.

The pipeline continuously ingests stock price data, processes it using Spark streaming, computes technical indicators, stores results in a columnar format, and visualizes trends on an interactive dashboard.

ğŸ¯ Objectives

Build a complete end-to-end data pipeline

Learn Structured Streaming concepts in Spark

Perform window-based aggregations

Store analytics data efficiently using Parquet

Visualize near real-time trends using Streamlit

Design a project that is resume and interview ready

ğŸ—ï¸ Architecture
Stock Data Source (Yahoo Finance - Free)
        â†“
Python Ingestion Service
        â†“
File-based Streaming (JSON events)
        â†“
PySpark Structured Streaming
        â†“
Parquet Storage
        â†“
Streamlit Dashboard

Why file-based streaming?

Free and simple to set up

Supported natively by Spark

Mimics Kafka-style micro-batches

Ideal for learning streaming fundamentals

ğŸ§° Tech Stack
Layer	Technology
Language	Python
Streaming Engine	PySpark (Structured Streaming)
Data Source	Yahoo Finance (near real-time)
Storage	Parquet
Visualization	Streamlit
Environment	Local (no cloud, no paid APIs)

ğŸ“‚ Project Structure
stock-market-analysis/
â”œâ”€â”€ ingestion/        # Data collection
â”œâ”€â”€ streaming/        # Spark streaming logic
â”œâ”€â”€ batch/            # Offline analytics
â”œâ”€â”€ analytics/        # Indicator logic
â”œâ”€â”€ visualization/    # Dashboard
â”œâ”€â”€ config/           # App & Spark configs
â”œâ”€â”€ utils/            # Helpers (logging, time)
â”œâ”€â”€ data/             # Raw, processed & checkpoints
â””â”€â”€ scripts/          # Run scripts


This structure follows industry-style separation of concerns.

ğŸ”„ Data Flow Explained

Ingestion

Python script fetches near real-time stock prices

Each price update is written as a JSON event

Streaming Processing

Spark Structured Streaming reads JSON files

Event time is extracted and parsed

Windowed aggregations (5-minute moving average) are computed

Storage

Aggregated results are written in Parquet format

Enables efficient querying and visualization

Visualization

Streamlit reads processed Parquet files

Interactive line charts show price trends

ğŸ“Š Analytics Implemented

5-minute windowed moving average of stock prices

Symbol-wise aggregation

Event-time based processing (not processing-time)

â–¶ How to Run the Project
1ï¸âƒ£ Create and activate virtual environment
python -m venv venv
source venv/bin/activate   # Linux / WSL
# OR
venv\Scripts\Activate.ps1  # Windows

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Start data ingestion
python ingestion/stock_data_collector.py

4ï¸âƒ£ Start Spark streaming (new terminal)
spark-submit streaming/stock_stream_processor.py

5ï¸âƒ£ Start visualization dashboard (new terminal)
streamlit run visualization/streamlit_app.py


Open in browser:

http://localhost:8501

âš  Limitations

Uses near real-time data (not tick-level exchange data)

File-based streaming instead of Kafka

Designed for learning and local execution

These trade-offs were made intentionally to keep the project free, transparent, and educational.

ğŸš€ Future Enhancements

Add more indicators (EMA, RSI, Bollinger Bands)

Integrate Kafka for real message streaming

Add alerting for price spikes

Containerize using Docker

Deploy dashboard to cloud

ğŸ’¬ How to Explain This Project in Interviews

â€œI built a near real-time stock market analytics pipeline using PySpark Structured Streaming. Since free real-time exchange feeds are restricted, I designed a file-based streaming ingestion system that simulates real streaming behavior. Spark processes the data using event-time windows, stores results in Parquet, and the trends are visualized using Streamlit.â€

ğŸ§‘â€ğŸ“ Author Notes

This project was built independently for learning purposes, focusing on core data engineering concepts rather than paid tools or managed services.